{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa5137e",
   "metadata": {},
   "source": [
    "The tensor object holds one pointer to the start of that contiguous block. The elements themselves are accessed via simple pointer arithmetic based on shape/stride, not through additional pointers per element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d852278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.]) torch.Size([4])\n",
      "tensor([[0., 0., 0., 0.]]) torch.Size([1, 4])\n",
      "tensor([[0., 0., 0., 0.]]) torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "points = torch.zeros(4)\n",
    "print(points, points.shape)\n",
    "\n",
    "points = points[None] # Adds a dimension of size 1, just like unsqueeze\n",
    "print(points, points.shape)\n",
    "\n",
    "points = torch.zeros(4)\n",
    "points = torch.unsqueeze(points, dim=0)\n",
    "print(points, points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82ba9e",
   "metadata": {},
   "source": [
    "``torch.unsqueeze`` is a **non-mutating operation**: An operation that returns a new object rather than modifying the original one. If I wanted it to be an in-place operation, I'd have to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df89df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.]]) torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "points = torch.zeros(4)\n",
    "points.unsqueeze_(dim=0)\n",
    "print(points, points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86c8e6",
   "metadata": {},
   "source": [
    "## Named tensors\n",
    "\n",
    "We need to remember the ordering of the dimensions of a tensor to perform indexing properly, and keeping track of that as tensors are operated on can be error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13efbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5323, 0.3062, 0.4296, 0.5832, 0.7785],\n",
      "         [0.9456, 0.3301, 0.3890, 0.3865, 0.4743],\n",
      "         [0.1099, 0.7306, 0.3771, 0.8148, 0.8417],\n",
      "         [0.0143, 0.9922, 0.7060, 0.4197, 0.2523],\n",
      "         [0.0888, 0.8428, 0.1962, 0.9237, 0.9444]],\n",
      "\n",
      "        [[0.6308, 0.7288, 0.0802, 0.0920, 0.4818],\n",
      "         [0.0716, 0.6737, 0.4062, 0.0832, 0.7225],\n",
      "         [0.4502, 0.6662, 0.9211, 0.9130, 0.7539],\n",
      "         [0.6602, 0.0678, 0.4497, 0.5164, 0.6099],\n",
      "         [0.2338, 0.6252, 0.4737, 0.9767, 0.3831]],\n",
      "\n",
      "        [[0.3814, 0.1284, 0.5853, 0.1698, 0.0966],\n",
      "         [0.2337, 0.1087, 0.2341, 0.1857, 0.3446],\n",
      "         [0.1534, 0.1133, 0.6370, 0.0355, 0.6777],\n",
      "         [0.4399, 0.0371, 0.8363, 0.3337, 0.5920],\n",
      "         [0.6145, 0.9361, 0.2971, 0.8338, 0.8945]]])\n"
     ]
    }
   ],
   "source": [
    "img_t = torch.rand(3, 5, 5) # random RGB image, shape (channels, rows, columns)\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722]) # weights for each channel, shape (channels,)\n",
    "print(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4d0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5148, 0.3878, 0.3650, 0.2817, 0.4523],\n",
      "        [0.4169, 0.3708, 0.3431, 0.2185, 0.5138],\n",
      "        [0.2378, 0.5034, 0.6451, 0.5878, 0.7578],\n",
      "        [0.3714, 0.3657, 0.6640, 0.4233, 0.4847],\n",
      "        [0.3124, 0.8014, 0.3223, 0.9114, 0.7407]])\n"
     ]
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "print(img_gray_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05a035",
   "metadata": {},
   "source": [
    "\"PyTorch will allow us to multiply things that are the same shape, as well as shapes where one operand is of size 1 in a given dimension\". This means:\n",
    "\n",
    "PyTorch can automatically \"stretch\" tensors along dimensions where one of the tensors has size 1, so that their shapes match for elementwise operations like multiplication. This is called **broadcasting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8287a661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20],\n",
       "        [60, 80]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])      # shape: (2, 2)\n",
    "b = torch.tensor([[10], [20]])          # shape: (2, 1)\n",
    "\n",
    "# PyTorch will broadcast b to match a. Broadcasting b to match a:\n",
    "# [[10, 10],\n",
    "#  [20, 20]]\n",
    "\n",
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a89ab",
   "metadata": {},
   "source": [
    "Broadcasting also appends leading dimensions of size 1 automatically. Leading dimensions are the ones on the left of a shape tuple. PyTorch (like NumPy) aligns shapes from the right. If two tensors have different number of dimensions, it automatically adds 1s on the left (i.e., leading 1s) to the smaller shape so broadcasting rules can apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fed7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5]) torch.Size([3])\n",
      "torch.Size([3, 1, 1])\n",
      "torch.Size([3, 5, 5])\n",
      "tensor([[0.5919, 0.5956, 0.1909, 0.2020, 0.5171],\n",
      "        [0.2691, 0.5598, 0.3901, 0.1551, 0.6425],\n",
      "        [0.3564, 0.6400, 0.7849, 0.8288, 0.7671],\n",
      "        [0.5069, 0.2621, 0.5321, 0.4827, 0.5325],\n",
      "        [0.2305, 0.6939, 0.4019, 0.9551, 0.5393]])\n"
     ]
    }
   ],
   "source": [
    "print(img_t.shape, weights.shape)\n",
    "\n",
    "unsqueezzed_weights = weights.unsqueeze(-1).unsqueeze(-1) # unsqueezing because we want to maintain the leading dimension\n",
    "print(unsqueezzed_weights.shape)\n",
    "\n",
    "weighted_img = img_t*unsqueezzed_weights\n",
    "print(weighted_img.shape)\n",
    "\n",
    "img_gray_weighted = weighted_img.sum(-3)\n",
    "print(img_gray_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d3754",
   "metadata": {},
   "source": [
    "This get messy very quickly, so PyTorch provides ``einsum`` to perform such operations in a more compact manner. ``torch.einsum('notation', tensors...)`` uses Einstein summation notation to describe how to combine the dimensions of multiple tensors using summation, multiplication, or both.\n",
    "\n",
    "In ```torch.einsum('input1_labels,input2_labels->output_labels', tensor1, tensor2)```:\n",
    "* The first part of the string (before the `->`) contains labels that correspond positionally to the dimensions of each input tensor, separated by commas.\n",
    "* The second part (after the `->`) tells how the result should look.\n",
    "* Labels with the same letter mean the corresponding dimensions will be multiplied and summed over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4020047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5919, 0.5956, 0.1909, 0.2020, 0.5171],\n",
       "        [0.2691, 0.5598, 0.3901, 0.1551, 0.6425],\n",
       "        [0.3564, 0.6400, 0.7849, 0.8288, 0.7671],\n",
       "        [0.5069, 0.2621, 0.5321, 0.4827, 0.5325],\n",
       "        [0.2305, 0.6939, 0.4019, 0.9551, 0.5393]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the notation (first arg.) - c: channels (img_t[0]), h: height (img_t[1]), w: width (img_t[2]), c: weights (weights[0]) | the output will drop the c dimension, by summing over it. \n",
    "img_gray_weighted_einsum = torch.einsum('chw,c->hw', img_t, weights)\n",
    "img_gray_weighted_einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5e57c",
   "metadata": {},
   "source": [
    "So, in ``torch.einsum('chw,c->hw', img_t, weights)`` we are saying:\n",
    "* `chw` applies to `img_t` → shape: `(3, 5, 5)`\n",
    "* `c` applies to `weights` → shape: `(3,)`\n",
    "* `->hw` says: output should have dimensions `h` and `w`, meaning **sum over** `c`.\n",
    "\n",
    "More on `einsum` can be found [here](https://rockt.ai/2018/04/30/einsum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24589355",
   "metadata": {},
   "source": [
    "There's a lot of bookkeeping involved in this, being error-prone, specially if the locations where tensors are created and manipulated are far apart in the code. To address this, PyTorch 1.3 added **named tensors** as an experimental feature, by giving factory functions such as `tensor` and `rand` a `names` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b0bdca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wsoar\\AppData\\Local\\Temp\\ipykernel_37900\\2371314847.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/core/TensorImpl.h:1938.)\n",
      "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "876729c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5323, 0.3062, 0.4296, 0.5832, 0.7785],\n",
      "         [0.9456, 0.3301, 0.3890, 0.3865, 0.4743],\n",
      "         [0.1099, 0.7306, 0.3771, 0.8148, 0.8417],\n",
      "         [0.0143, 0.9922, 0.7060, 0.4197, 0.2523],\n",
      "         [0.0888, 0.8428, 0.1962, 0.9237, 0.9444]],\n",
      "\n",
      "        [[0.6308, 0.7288, 0.0802, 0.0920, 0.4818],\n",
      "         [0.0716, 0.6737, 0.4062, 0.0832, 0.7225],\n",
      "         [0.4502, 0.6662, 0.9211, 0.9130, 0.7539],\n",
      "         [0.6602, 0.0678, 0.4497, 0.5164, 0.6099],\n",
      "         [0.2338, 0.6252, 0.4737, 0.9767, 0.3831]],\n",
      "\n",
      "        [[0.3814, 0.1284, 0.5853, 0.1698, 0.0966],\n",
      "         [0.2337, 0.1087, 0.2341, 0.1857, 0.3446],\n",
      "         [0.1534, 0.1133, 0.6370, 0.0355, 0.6777],\n",
      "         [0.4399, 0.0371, 0.8363, 0.3337, 0.5920],\n",
      "         [0.6145, 0.9361, 0.2971, 0.8338, 0.8945]]],\n",
      "       names=('channels', 'rows', 'columns'))\n",
      "tensor([[[0.2126]],\n",
      "\n",
      "        [[0.7152]],\n",
      "\n",
      "        [[0.0722]]], names=('channels', 'rows', 'columns'))\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "weights_named_aligned = weights_named.align_as(img_named)\n",
    "print(img_named)\n",
    "print(weights_named_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada937bf",
   "metadata": {},
   "source": [
    "If we try to combine dimensions with different names, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4bf7805",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gray_named \u001b[38;5;241m=\u001b[39m (\u001b[43mimg_named\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mweights_named\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "gray_named = (img_named[2]*weights_named).sum('channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57d170",
   "metadata": {},
   "source": [
    "Outside functions that operate on named tensors we need to drop the names by renaming them to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fcaff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_with_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
